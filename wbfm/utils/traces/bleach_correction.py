import logging

import numpy as np
import pandas as pd
import scipy
import sklearn
from lmfit.models import ExponentialModel, ConstantModel
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler


def detrend_exponential(y_with_nan):
    """
    Bleach correction via simple exponential fit, subtraction, and re-adding the mean

    Uses np.polyfit on np.log(y), with errors weighted back to the data space. See:

    https://stackoverflow.com/questions/3433486/how-to-do-exponential-and-logarithmic-curve-fitting-in-python-i-found-only-poly

    Parameters
    ----------
    y_with_nan

    Returns
    -------

    """

    ind = np.where(~np.isnan(y_with_nan))[0]
    t = np.squeeze(StandardScaler(copy=False).fit_transform(ind.reshape(-1, 1)))
    y = y_with_nan[ind]
    y_log = np.log(y)

    fit_vars = np.polyfit(t, y_log, 1)#, w=np.sqrt(y))

    # Subtract in the original data space
    y_fit = np.exp(fit_vars[0]) * np.exp(t*fit_vars[1])
    y_corrected = y - y_fit + np.mean(y)

    return ind, y_corrected


def detrend_exponential_lmfit(y_with_nan, x=None, ind_subset=None, restore_mean_value=False, use_const=False):
    """
    Bleach correction via simple exponential fit, division, and (optionally) re-adding the mean

    Uses a direct solver, not a linear regression (does not take the log of the points).

    Parameters
    ----------
    x: optional; time-like vector
    y_with_nan: values to fit
    restore_mean_value
    use_const: whether to add a constant to the fit (extra parameter), or use a raw exponential
        NOTE: if the exponential is very shallow (i.e. almost constant), this term can destabilize the fit

    Returns
    -------

    """
    original_mean = np.nanmean(y_with_nan)

    exp_mod = ExponentialModel(prefix='exp_')
    model = exp_mod
    if use_const:
        const_mod = ConstantModel(prefix='const_')
        model = model + const_mod
    if ind_subset is None:
        ind_subset = np.where(~np.isnan(y_with_nan))[0]
    if x is None:
        x = ind_subset
    else:
        x = x[ind_subset]
    y = y_with_nan[ind_subset]
    out = None

    try:
        pars = exp_mod.guess(y, x=x)
        if use_const:
            pars += const_mod.guess(y, x=x)
        out = model.fit(y, pars, x=x)

        comps = out.eval_components(x=x)
        y_fit = comps['exp_']
        if use_const:
            y_fit += comps['const_']
        y_corrected = y / y_fit

        y_corrected_with_nan = np.empty_like(y_with_nan)
        y_corrected_with_nan[:] = np.nan
        y_corrected_with_nan[ind_subset] = y_corrected
        flag = True

    except (TypeError, ValueError):
        # Occurs when there are too few input points
        logging.warning("Exponential fit failed due to too few points; returning uncorrected values")
        y_corrected_with_nan, y_fit = y_with_nan, y_with_nan
        flag = False

    if out is None or not out.errorbars:# or 0 in y_fit:
        # Crude measurement of bad convergence, even if it didn't error out
        # logging.warning("Exponential fit failed; returning uncorrected values")
        # plt.scatter(x, y, color='gray', label='data')
        # plt.plot(x, y_fit, label='best fit')
        # plt.legend()
        # plt.show()
        y_corrected_with_nan, y_fit = y_with_nan, np.mean(y_with_nan) * np.ones_like(y_with_nan)
        flag = False

    if restore_mean_value:
        y_corrected_with_nan *= original_mean / np.nanmean(y_corrected_with_nan)

    return y_corrected_with_nan, (y_fit, out, flag)


def detrend_exponential_iter(trace, max_iters=100, convergence_threshold=0.01,
                             low_quantile=0.15, high_quantile=0.85, **kwargs):
    """
    Similar to detrend_exponential_lmfit, but it tries to remove outliers and high-activity points

    low/high_quantile: how many percent of the data should be excluded at bottom/top

    Parameters
    ----------
    trace
    convergence_threshold - stop if L2 norm changes by less than this
    low_quantile - per iteration, remove this bottom percentile
    high_quantile - per iteration, remove this bottom percentile

    Returns
    -------

    """
    y_full = trace
    ind_iter = np.where(~np.isnan(y_full))[0]
    y_detrend, num_iter = None, 0

    for num_iter in range(max_iters):
        y_detrend, fit_results = detrend_exponential_lmfit(y_full, ind_subset=ind_iter, **kwargs)
        y_fit = fit_results[0]
        y_fit_last = y_fit

        ind_not_too_small = np.nanquantile(y_detrend, low_quantile) < y_detrend
        ind_not_too_large = y_detrend < np.nanquantile(y_detrend, high_quantile)
        ind_iter = np.where(np.logical_and(ind_not_too_small, ind_not_too_large))[0]
        if scipy.spatial.distance.euclidean(y_fit, y_fit_last) <= convergence_threshold:
            break
    return y_detrend, num_iter


def full_lm_with_windowed_regression_vol(project_data, neuron, window_size=5):
    """" gives back corrected trace for the selected neuron
    calculates alpha for every timepoint considering all neighbours with distance < window size"""

    num_timepoints = project_data.red_traces.shape[0]

    green = np.array(project_data.green_traces[neuron]["intensity_image"])
    vol = np.array(project_data.green_traces[neuron]["area"])
    red = np.array(project_data.red_traces[neuron]["intensity_image"])
    remove_nan = np.logical_and(np.invert(np.isnan(green)), np.invert(np.isnan(vol)))
    green = green[remove_nan]
    vol = vol[remove_nan]
    red = red[remove_nan]

    alpha_green = []
    for i in range(window_size, len(red) - window_size - 1):
        y = green[i - window_size:i + window_size]
        x = vol[i - window_size:i + window_size].reshape(-1, 1)
        model = sklearn.linear_model.LinearRegression(fit_intercept=False)
        model.fit(x, y)
        alpha_green.append(model.coef_)

    alpha_red = []
    for i in range(window_size, len(red) - window_size - 1):
        y = red[window_size:len(red) - window_size - 1]
        x = vol[window_size:len(red) - window_size - 1].reshape(-1, 1)
        model = sklearn.linear_model.LinearRegression(fit_intercept=False)
        model.fit(x, y)
        alpha_red.append(model.coef_)

    red_corrected = red[window_size:len(red) - window_size - 1] - np.array(alpha_red).flatten() * vol[window_size:len(
        red) - window_size - 1]
    green_corrected = green[window_size:len(red) - window_size - 1] - np.array(alpha_green).flatten() * vol[
                                                                                                        window_size:len(
                                                                                                            red) - window_size - 1]

    vol = project_data.red_traces[neuron]["area"][remove_nan][window_size:len(red) - window_size - 1]
    x = project_data.red_traces[neuron]["x"][remove_nan][window_size:len(red) - window_size - 1]
    y = project_data.red_traces[neuron]["y"][remove_nan][window_size:len(red) - window_size - 1]
    z = project_data.red_traces[neuron]["z"][remove_nan][window_size:len(red) - window_size - 1]
    t = np.array(range(num_timepoints))[remove_nan][window_size:len(red) - window_size - 1]
    X = [red_corrected, vol, x, y, z, t]

    X = np.c_[np.array(X).T]
    model = sklearn.linear_model.LinearRegression()
    model.fit(X, green_corrected)
    x_pred = model.predict(X)

    res = green_corrected - x_pred
    return res


def bleach_correct_gaussian_moving_average(y: pd.Series, std=300, subtract=True) -> pd.Series:
    """
    Subtracts or divides traces from a gaussian filtered version of the trace

    See filter_gaussian_moving_average
    """
    y_filt = y.rolling(center=True, window=800, win_type='gaussian', min_periods=1).mean(std=std)
    if subtract:
        return y - y_filt + y_filt.mean()
    else:
        return y / y_filt
