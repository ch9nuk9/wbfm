{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d810089d",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "Starting point: neuron detections\n",
    "\n",
    "1. Match features (general, not just neurons)\n",
    "2. Loop over neurons and build local affine transformations\n",
    "3. Combine local transformations into a global vector field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab7aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loosely inspired from caiman, but they only do translation, not rotation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from skimage.transform import warp\n",
    "import numpy as np\n",
    "import zarr\n",
    "import napari\n",
    "from DLC_for_WBFM.utils.postprocessing.base_cropping_utils import get_crop_coords3d\n",
    "from DLC_for_WBFM.utils.feature_detection.utils_rigid_alignment import calc_warp_ECC\n",
    "from tqdm.auto import tqdm\n",
    "from DLC_for_WBFM.utils.video_and_data_conversion.import_video_as_array import get_single_volume\n",
    "from DLC_for_WBFM.utils.projects.utils_project import load_config\n",
    "from pathlib import Path\n",
    "from DLC_for_WBFM.utils.projects.utils_project import safe_cd\n",
    "import cv2\n",
    "from DLC_for_WBFM.utils.feature_detection.visualization_tracks import visualize_tracks\n",
    "from DLC_for_WBFM.utils.preprocessing.utils_tif import PreprocessingSettings\n",
    "from DLC_for_WBFM.utils.preprocessing.utils_tif import perform_preprocessing\n",
    "from DLC_for_WBFM.utils.feature_detection.utils_features import build_features_and_match_2volumes, extract_map1to2_from_matches\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "%load_ext autoreload\n",
    "import itertools\n",
    "import pickle\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67477d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_path = r\"Y:\\shared_projects\\wbfm\\dlc_stacks\\Charlie-worm3-long\\project_config.yaml\"\n",
    "cfg = load_config(project_path)\n",
    "\n",
    "red_btf = cfg['red_bigtiff_fname']\n",
    "num_z = cfg['dataset_params']['num_slices']\n",
    "\n",
    "project_dir = Path(project_path).parent\n",
    "\n",
    "with safe_cd(project_dir):\n",
    "    train_fname = Path(cfg['subfolder_configs']['training_data'])\n",
    "    train_cfg = dict(load_config(train_fname))\n",
    "    \n",
    "    p_fname = train_cfg['preprocessing_config']\n",
    "    p = PreprocessingSettings.load_from_yaml(p_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f905ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "i_ref = 300\n",
    "ref_frame_raw = get_single_volume(red_btf, i_ref, num_z, 0.15)\n",
    "\n",
    "i_test = 301\n",
    "test_frame_raw = get_single_volume(red_btf, i_test, num_z, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc5886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do z-alignment (preprocessing)\n",
    "\n",
    "ref_frame = perform_preprocessing(ref_frame_raw, p)\n",
    "test_frame = perform_preprocessing(test_frame_raw, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5c79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get segmentation\n",
    "# fname = r\"Y:\\shared_projects\\wbfm\\dlc_stacks\\Charlie-worm3-long\\4-traces\\reindexed_masks.zarr\"\n",
    "fname = r\"Y:\\shared_projects\\wbfm\\dlc_stacks\\Charlie-worm3-long\\1-segmentation\\masks_1500.zarr\"\n",
    "z = zarr.open(fname)\n",
    "\n",
    "ref_seg = np.array(z[i_ref,...])\n",
    "test_seg = np.array(z[i_test,...])\n",
    "\n",
    "# And metadata\n",
    "fname = r\"Y:\\shared_projects\\wbfm\\dlc_stacks\\Charlie-worm3-long\\1-segmentation\\metadata_1500.pickle\"\n",
    "with open(fname, 'rb') as f:\n",
    "    seg_metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c864c",
   "metadata": {},
   "source": [
    "# Step 0: Apply a pre-rotation (global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99254b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_2d, rotated_frame, zxy0, zxy1 = get_warp_via_features_from_imgs(ref_frame, test_frame, apply_to_slices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dde46e",
   "metadata": {},
   "source": [
    "# Steps 1 and 2:  match features and build local flow fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afe383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a flow field from the rotation matrix\n",
    "def flow_field_from_matrix(shape, A):\n",
    "    if A is None or len(A)==0:\n",
    "        return None\n",
    "    x, y = shape\n",
    "    xx, yy = np.arange(x), np.arange(y)\n",
    "    A = cv2.invertAffineTransform(A)\n",
    "    \n",
    "    all_xy = np.array([[yy[i_y], xx[i_x], 1] for i_x, i_y in itertools.product(range(x), range(y))])\n",
    "    flow_long = A@all_xy.T\n",
    "    flow0 = np.reshape(flow_long[0,:], (x, y), order='A')\n",
    "    flow1 = np.reshape(flow_long[1,:], (x, y), order='A')\n",
    "    flow = np.stack([flow0, flow1], axis=2).astype('float32')\n",
    "    return flow\n",
    "\n",
    "# Redo feature matching on the crop\n",
    "def get_warp_via_features_from_imgs(ref_frame, test_frame, apply_to_slices=False):\n",
    "    # 3d\n",
    "    all_locs0, all_locs1, all_kp0, all_kp1, all_matches = build_features_and_match_2volumes(\n",
    "        ref_frame, test_frame, use_GMS=True, verbose=0, start_plane=5, matches_to_keep=0.5\n",
    "    )\n",
    "    if len(all_locs0) == 0:\n",
    "#         print(\"No matches found\")\n",
    "        return all_kp0, all_kp1, None, None\n",
    "    zxy0 = np.array(all_locs0[:,1:], dtype='float64')\n",
    "    zxy1 = np.array(all_locs1[:,1:], dtype='float64')\n",
    "    # Try skimage\n",
    "    trans = transform.estimate_transform('euclidean', zxy1, zxy0)\n",
    "    h_2d = trans.params[:2,:]\n",
    "    \n",
    "    rotated_frame = None\n",
    "    if h_2d is not None:\n",
    "        if apply_to_slices:\n",
    "            rotated_frame = np.zeros_like(test_frame)\n",
    "            for i, f in enumerate(test_frame):\n",
    "                sz = (f.shape[1], f.shape[0])\n",
    "                out = cv2.warpAffine(f, h_2d, dsize=sz)\n",
    "                rotated_frame[i,...] = out\n",
    "    else:\n",
    "        print(\"No rotation found\")\n",
    "    return h_2d, rotated_frame, zxy0, zxy1\n",
    "\n",
    "def vector_field_to_flow(vf):\n",
    "    h, w = vf.shape[:2]\n",
    "    flow = vf.copy()\n",
    "    flow = -flow\n",
    "    flow[:,:,0] += np.arange(w)\n",
    "    flow[:,:,1] += np.arange(h)[:,np.newaxis]\n",
    "    return flow\n",
    "\n",
    "def flow_to_vector_field(flow):\n",
    "    h, w = flow.shape[:2]\n",
    "    vf = flow.copy()\n",
    "    vf[:,:,0] -= np.arange(w)\n",
    "    vf[:,:,1] -= np.arange(h)[:,np.newaxis]\n",
    "    vf = -vf\n",
    "    return vf\n",
    "\n",
    "def warp_flow(img, flow, apply_directly=False):\n",
    "    # From:\n",
    "    # https://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py#L50-L56\n",
    "    # apply_directly should be False if the output is from \n",
    "    if not apply_directly:\n",
    "        flow = vector_field_to_flow(flow)\n",
    "    res = cv2.remap(img, flow, None, cv2.INTER_LINEAR)\n",
    "    return res\n",
    "\n",
    "def get_cropped_image(centroid, test_frame):\n",
    "    # Get a cropped cube in both volumes\n",
    "    z, x, y = get_crop_coords3d(centroid, crop_sz=(9, 128, 128), clip_sz=None)\n",
    "\n",
    "    test_crop_full_size = np.zeros_like(test_frame)\n",
    "#     test_crop_full_size[z[0]:z[-1], x[0]:x[-1], y[0]:y[-1]] = test_frame[z[0]:z[-1], x[0]:x[-1], y[0]:y[-1]]\n",
    "    test_crop_full_size[:, x[0]:x[-1], y[0]:y[-1]] = test_frame[:, x[0]:x[-1], y[0]:y[-1]]\n",
    "    return test_crop_full_size\n",
    "\n",
    "\n",
    "def get_flow_field_from_centroid(centroid, ref_frame, test_frame):\n",
    "    cropped_test_frame = get_cropped_image(centroid, test_frame)\n",
    "    h_2d, _, _, _ = get_warp_via_features_from_imgs(ref_frame, cropped_test_frame)\n",
    "    if h_2d is None:\n",
    "        return None, None\n",
    "    flow = flow_field_from_matrix(cropped_test_frame.shape[1:], h_2d)\n",
    "    weights = cropped_test_frame > 0\n",
    "    return flow, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b776842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badef04c8f184d2a85cc6e063e4ab754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_flows, all_weights = [], []\n",
    "num_neurons = len(seg_metadata[i_ref])\n",
    "for i_seg in tqdm(range(num_neurons), total=num_neurons):\n",
    "    # Get a centroid\n",
    "    centroid = seg_metadata[i_ref].iloc[i_seg]['centroids']\n",
    "    flow, weights = get_flow_field_from_centroid(centroid, ref_frame, rotated_frame)\n",
    "    all_flows.append(flow)\n",
    "    all_weights.append(weights)\n",
    "    \n",
    "    if i_seg > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec391b0",
   "metadata": {},
   "source": [
    "# Step 3: Combine the transforms into a global field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc201f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_func(x, thresh):\n",
    "    if x == 0 or x > thresh:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1.0/x\n",
    "\n",
    "\n",
    "def combine_flow_fields(all_flows, all_weights, outliers, err_thresh=30):\n",
    "    # Weight each flow field by how accurate the overlap is\n",
    "    flow = np.zeros_like(all_flows[0])\n",
    "    overlapping_weights = np.ones(flow.shape[:-1])\n",
    "    for i, (w, f) in tqdm(enumerate(zip(all_weights, all_flows))):\n",
    "        if i in outliers or f is None or len(f)==0:\n",
    "            continue\n",
    "        mask = np.max(w,axis=0)\n",
    "        # Get both raw patches\n",
    "        centroid = seg_metadata[i_ref].iloc[i]['centroids']\n",
    "        i_slice = int(centroid[0])\n",
    "        this_ref = np.where(mask, ref_frame[i_slice,...], 0)\n",
    "        this_test = np.where(mask, test_frame[i_slice,...], 0)\n",
    "        \n",
    "\n",
    "        w_dist = mask\n",
    "#         w_dist = distance_transform_edt(mask)\n",
    "        overlapping_weights += w_dist\n",
    "#         flow[...,0] += np.where(w_binary, f[...,0], 0)\n",
    "#         flow[...,1] += np.where(w_binary, f[...,1], 0)\n",
    "        flow[...,0] += np.multiply(f[...,0], w_dist)\n",
    "        flow[...,1] += np.multiply(f[...,1], w_dist)\n",
    "    tmp = np.stack([overlapping_weights, overlapping_weights], axis=-1)\n",
    "    flow = np.divide(flow, tmp)\n",
    "    return flow, overlapping_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c1c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1ac392f1274ccd9d03c4fc8e392a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outliers = []\n",
    "# outliers = [0, 88]\n",
    "# outliers = [13, 17, 32, 74, 88, 89, 93, 94, 96, 106, 108, 111, 121, 123, 129, 149, 153, 155, 157, 159, 162]\n",
    "flow, overlapping_weights = combine_flow_fields(all_flows, all_weights, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a9b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea443d83",
   "metadata": {},
   "source": [
    "# Step 4: Check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75545e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860b114a5f8e40fc8a8a38dd6922b5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='i', max=12), IntSlider(value=15, description='z', max=31â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(i, z)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK\n",
    "i_slice = 18\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "def f(i, z):\n",
    "    plt.figure(figsize=(45,25))\n",
    "    centroid = seg_metadata[i_ref].iloc[i]['centroids']\n",
    "    \n",
    "    i_slice = int(centroid[0])\n",
    "    \n",
    "    plt.imshow(warp_flow(rotated_frame[z,...], all_flows[i], apply_directly=True), alpha=0.5, cmap=\"Reds\")\n",
    "    \n",
    "    tmp = get_cropped_image(centroid, ref_frame)\n",
    "    plt.imshow(tmp[z,...], alpha=0.5, cmap=\"Greens\")\n",
    "    plt.title(f\"Overlay after global and local affine transformations (slice {i_slice})\")\n",
    "\n",
    "interact(f, i=(0, len(all_flows)), z=(0,rotated_frame.shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "839b53b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rotated_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-77036eed6459>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotated_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_slice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Reds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_slice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Greens\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Overlay after affine transformation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rotated_frame' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHECK\n",
    "i_slice = 17\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.imshow(rotated_frame[i_slice,...], alpha=0.5, cmap=\"Reds\")\n",
    "plt.imshow(ref_frame[i_slice,...], alpha=0.5, cmap=\"Greens\")\n",
    "plt.title(\"Overlay after affine transformation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f4efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation]",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
